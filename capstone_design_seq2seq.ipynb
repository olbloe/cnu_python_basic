{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOc+y4hSY9CdxJ6+5QzUfxN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olbloe/cnu_python_basic/blob/main/capstone_design_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋으로 사용할 데이터 전처리"
      ],
      "metadata": {
        "id": "aVGDOplrn-GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "tNm7TPn1vmGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I5VTB12vbG9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " 데이터 전처리\n",
        "'''\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "CHANGE_FILTER = re.compile(FILTERS) # 미리 Complie\n",
        "PAD, PAD_INDEX = \"<PAD>\", 0 # 패딩 토큰\n",
        "STD, STD_INDEX = \"<SOS>\", 1 # 시작 토큰\n",
        "END, END_INDEX = \"<END>\", 2 # 종료 토큰\n",
        "UNK, UNK_INDEX = \"<UNK>\", 3 # 사전에 없음\n",
        "MARKER = [PAD,STD,END,UNK]\n",
        "MAX_SEQUNECE = 25"
      ],
      "metadata": {
        "id": "giyfrFFrvgQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data reading\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path,header=0)\n",
        "    question, answer = list(df['Q']),list(df['A'])\n",
        "    return question, answer"
      ],
      "metadata": {
        "id": "O3haR-OwvgOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "def data_tokenizer(data):\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        # 미리 컴파일한 특수문자를 제거하는 코드\n",
        "        sentence = re.sub(CHANGE_FILTER,\"\",sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word) \n",
        "    # 공백 기준으로 단어를 나눠서 Return\n",
        "    return [word for word in words if word]"
      ],
      "metadata": {
        "id": "ksYObVOAvgHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분리 \n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer= Okt()\n",
        "    results = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ','')))\n",
        "        results.append(morphlized_seq)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "B_LOGCgHvgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전을 불러오는 함수\n",
        "def load_vocabulary(path, vocab_path):\n",
        "    vocabulary_list = []\n",
        "    # vocab path가 없고 -- 단어 사전파일이 없고\n",
        "    if not os.path.exists(vocab_path):\n",
        "        # Raw데이터를 불러와서 사전을 만든다.\n",
        "        # if (os.path.exists(path)):\n",
        "        df = pd.read_csv(path,encoding='utf-8')\n",
        "        question, answer = list(df['Q']),list(df['A'])\n",
        "        data = []\n",
        "        data.extend(question)\n",
        "        data.extend(answer)\n",
        "        # Tokenizing \n",
        "        words = data_tokenizer(data)\n",
        "        words = list(set(words))\n",
        "        words[:0] = MARKER # 사전에 정의한 토큰을 단어 리스트 앞에 추가\n",
        "            # print(vocab_path)\n",
        "        # print(words)\n",
        "        with open(vocab_path, 'w', encoding = 'utf-8') as vocabulary_file:\n",
        "            for word in words:\n",
        "                # print(word)\n",
        "                vocabulary_file.write(word + '\\n')\n",
        "\n",
        "    \n",
        "        \n",
        "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            # print(line)\n",
        "            vocabulary_list.append(line.strip())\n",
        "    # print(vocabulary_list) \n",
        "    word2idx, idx2word = make_vocabulary(vocabulary_list)\n",
        "    \n",
        "    return word2idx, idx2word, len(word2idx)"
      ],
      "metadata": {
        "id": "E1PG7ylwvgDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vocabulary(vocabulary_list):\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary_list)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocabulary_list)}\n",
        "\n",
        "    return word2idx, idx2word"
      ],
      "metadata": {
        "id": "CJe0MpD6vf7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더와 디코더 부분 처리하기\n",
        "def enc_processing(value, dictionary):\n",
        "    sequences_input_index = []\n",
        "    sequences_length = []\n",
        "\n",
        "    for sequence in value :\n",
        "        sequence = re.sub(CHANGE_FILTER,\"\",sequence)\n",
        "        sequence_index = []\n",
        "        \n",
        "        for word in sequence.split(): # 공백 기준으로 word를 구분\n",
        "            if dictionary.get(word) is not None : # 사전에 있으면\n",
        "                sequence_index.extend([dictionary[word]]) # index 값 쓰고\n",
        "            else:\n",
        "                sequence_index.extend([dictionary[UNK]])\n",
        "        # 길이 제한\n",
        "        if len(sequence_index) > MAX_SEQUNECE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
        "\n",
        "        sequences_length.append(len(sequence_index)) # 이 문장의 길이 저장\n",
        "        # Padding 추가\n",
        "        # \"안녕\"  → \"안녕,<PAD>,<PAD>,<PAD>,<PAD>\"\n",
        "        \n",
        "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
        "        \n",
        "        sequences_input_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_input_index), sequences_length"
      ],
      "metadata": {
        "id": "dsEQkyGevfng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder input\n",
        "\n",
        "def dec_output_processing(value, dictionary):\n",
        "    sequences_output_index = []\n",
        "    sequences_length = []\n",
        "\n",
        "    for sequence in value:\n",
        "        sequence = re.sub(CHANGE_FILTER,\"\",sequence)\n",
        "        sequence_index = []\n",
        "        # 앞부분에 시작을 알리는 토큰 넣기\n",
        "        sequence_index = [dictionary[STD]]+[dictionary[word] for word in sequence.split()]\n",
        "\n",
        "        if len(sequence_index) > MAX_SEQUNECE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
        "\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
        "\n",
        "        sequences_output_index.append(sequence_index)\n",
        "    return np.asarray(sequences_output_index), sequences_length"
      ],
      "metadata": {
        "id": "nXBT5cjUwOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 Target 값 전처리\n",
        "def dec_target_processing(value,dictionary):\n",
        "    sequences_target_index = []\n",
        "    for sequence in value :\n",
        "        sequence = re.sub(CHANGE_FILTER,\"\", sequence)\n",
        "        sequence_index = [dictionary[word] for word in sequence.split() ]\n",
        "        if len(sequence_index)>= MAX_SEQUNECE:\n",
        "            # 이부분이 Decoder 입력값 전처리와 다른점\n",
        "            sequence_index = sequence_index[:MAX_SEQUNECE-1] + [dictionary[END]] #마지막에 END xhzms\n",
        "        else :\n",
        "            sequence_index += [dictionary[END]]\n",
        "\n",
        "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
        "        sequences_target_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_target_index)"
      ],
      "metadata": {
        "id": "A3dXgk9mwR1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    PATH = 'C:/Users/Eojin/datasets/token_sentence.csv'\n",
        "    VOCAB_PATH = 'data_in/vocabulary.txt'\n",
        "    # 데이터 부르기\n",
        "    inputs, outputs = load_data(PATH)\n",
        "    # 단어 사전 부르기\n",
        "    # 토크나이저를 사용하여 처리하도록 변경하기\n",
        "    char2idx, idx2char, vocab_size = load_vocabulary(PATH,VOCAB_PATH)\n",
        "    # print(char2idx)\n",
        "\n",
        "    # encoder/decoder input /target\n",
        "    index_inputs, input_seq_len = enc_processing(inputs, char2idx)\n",
        "    index_outputs, output_seq_len = dec_output_processing(outputs, char2idx)\n",
        "    index_targets =  dec_target_processing(outputs, char2idx)\n",
        "\n",
        "    data_configs = {}\n",
        "    data_configs['char2idx'] =char2idx\n",
        "    data_configs['idx2char'] = idx2char\n",
        "    data_configs['vocab_size'] = vocab_size\n",
        "    data_configs['pad_symbol'] = PAD\n",
        "    data_configs['std_symbol'] = STD\n",
        "    data_configs['end_symbol'] = END\n",
        "    data_configs['unk_symbol'] = UNK\n",
        "\n",
        "    DATA_IN_PATH = './data_in/'\n",
        "    np.save(open(DATA_IN_PATH+'train_inputs.npy','wb'), index_inputs)\n",
        "    np.save(open(DATA_IN_PATH+'train_outputs.npy','wb'), index_outputs)\n",
        "    np.save(open(DATA_IN_PATH+'train_targets.npy','wb'), index_targets)\n",
        "\n",
        "    json.dump(data_configs, open(DATA_IN_PATH+'data_configs.json','w'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "quYXKlz4wUDv",
        "outputId": "4f286882-eb4d-4cb6-8d86-5646006ffc5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1ecfce45e02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mVOCAB_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data_in/vocabulary.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 데이터 부르기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 단어 사전 부르기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 토크나이저를 사용하여 처리하도록 변경하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ca83140ecdb1>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_in/ChatBotData.csv'"
          ]
        }
      ]
    }
  ]
}